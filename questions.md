1. What are "ensemble methods" in general?
2. What is the difference between hard-voting and soft-voting?
3. How does Bootstrap Aggregation work?
4. What problem is Bagging primarily designed to overcome?
5. What kind of classifier can you use with a Bootstrap Classifier in sklearn's BaggingClassifier?
6. What is "out of bag" evaluation?
7. What is the difference between a BaggingClassifier with a decision tree and a RandomForest?  (random subset of features)
8. What is one limitation with RandomForests?
9. Explain the differences between Boosting and Bagging.
10. What is the difference between AdaBoost and GradientBoosting?
11. If I want to use a small learning in Gradient Boosting, how should I change the number of trees that get built?
12. If I have a machine with a lot of cores, which will be faster, RandomForests or GradientBoosting?  Why?
13. Is XGBoost more similar to GradientBoosting or AdaBoost?
14. Name one or two advantages of XGBoost over other boosting methods.
